# -*- coding: utf-8 -*-
"""dgcca.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15L_7jxf0KH81UjAO6waIbQso1nqML0kD

@author : Madiha Qureshi

# Deep Generalized Cannonical Correlation Analysis Implementaion for 3 views

**cca-zoo library** - It is a collection of variants of canonical correlation analysis for multiview data.
Here, cca-zoo package is used for the implemntaion of GCCA (Generalized Cannonical Correlation Analysis). It contatins the differentiable GCCA Loss method that takes the outputs of each view's network and solves the GCCA eigenproblem as in the research paper.
"""

#install the cca-zoo package
#!pip install cca-zoo

#importing required libraries -
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim 
from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler
from cca_zoo.objectives import GCCA as GCCA_Loss
from cca_zoo.wrapper import Wrapper as GCCA
#Loss() method takes the outputs of each view's network and solves the generalized CCA eigenproblem

"""##  Class DNN : Creates a new Deep Neural Network

**Parameters** :


*    **layer_size** - It is the list of size of each layer in the DNN staring from the input layer
*   **activation** - The type of activation function to be used. Choose from 'relu' , 'tanh' , 'sigmoid' . By default, sigmoid.

**Methods**

  

*   **forward(self, l)** : forward propogates input l into the DNN and returns the output
"""

class DNN(nn.Module):
    def __init__(self, layer_size, activation):
        super(DNN, self).__init__()
        layers = []
        self.activation = activation
        
        # Defaults to sigmoid 
        if self.activation == 'relu':
          self.activation_func = nn.RelU()
        elif self.activation == 'tanh':
          self.activation_func = nn.Tanh()
        elif self.activation == 'sigmoid':
          self.activation_func = nn.Sigmoid()
        else:
          self.activation_func = nn.Sigmoid()

        for l_id in range(len(layer_size) - 1):
            if l_id == len(layer_size) - 2: #second last layer
                layers.append(nn.Sequential(
                    nn.BatchNorm1d(num_features=layer_size[l_id], affine=False),
                    nn.Linear(layer_size[l_id], layer_size[l_id + 1]),
                ))
            else: #all other layers
                layers.append(nn.Sequential(
                    nn.Linear(layer_size[l_id], layer_size[l_id + 1]), 
                    self.activation_func,
                    nn.BatchNorm1d(num_features=layer_size[l_id + 1], affine=False),                                     
                ))

        self.layers = nn.ModuleList(layers)

    def forward(self, l):
        for layer in self.layers:
            l = layer(l)
        return l

"""## Class : DGCCA_architecture - Defines the architecture for three DNNs

**Parameters**


*   **layer_size1 , layer_size2 , layer_size3** : list of sizes of each layer of first, second and third DNN(view) respectively.

**Methods**

  

*   **forward(self, x1, x2, x3)** : forward propogates x1 into the first DNN,x2 into the second DNN and x3 into the third DNN and returns the outputs.
"""

class DGCCA_architecture(nn.Module): # for three views
    def __init__(self, layer_size1, layer_size2, layer_size3, activation):
        super(DGCCA_architecture, self).__init__()
        self.activation = activation
        self.model1 = DNN(layer_sizes1,activation)
        self.model2 = DNN(layer_sizes2,activation)
        self.model3 = DNN(layer_sizes2,activation)

    def forward(self, x1, x2, x3):
        output1 = self.model1(x1)
        output2 = self.model2(x2)
        output3 = self.model3(x3)

        return output1, output2, output3

"""## Class DGCCA : Implements the DGCCA Algorithm

**Parameters**


*   **architecture** : object of DGCCA_architecture class.
*   **gcca_wrraper** : from cca-zoo package to implement gcca
*   **learning_rate** : learning_rate of the network
*   **epoch_num** :How long to train the model.
*   **batch_size** : Number of example per minibatch.
*   **reg_param** :  the regularization parameter of the network
*   **out_size** : the size of the new space learned by the model (number of the new features)


**Methods**

  

*   **fit(self, train_x1, train_x2, train_x3, test_x1, test_x2, test_x3)** - trains and tests the networks batch-wise. Also, back propogates the ggca loss. First three parameters are the training set for each view respectively. The last three parameters are the testing set for each view respectively
*   **_get_outputs(self, x1, x2, x3)** - returns gcca loss and output as both lists for given inputs x1, x2, x3 for view first, second, third respectively.
*   **test(self, x1, x2, x3)** - returns gcca loss mean and output as list for given inputs x1, x2, x3 for view first, second, third respectively.
*  **train_gcca(self, x1, x2, x3)** - uses the gcca.fit() from cca zoo on given inputs x1,x2,x3
"""

class DGCCA(nn.Module):
  def __init__(self, architecture, learning_rate, epoch_num, batch_size, reg_par, out_size:int):
        super(DGCCA, self).__init__()
        self.arch = architecture
        self.learning_rate = learning_rate
        self.reg_par = reg_par
        # Stochastic Gradient Descent used as optimizer like in the research paper
        self.optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=reg_par)
        self.epoch_num = epoch_num
        self.batch_size = batch_size
        self.out_size = out_size
        # The GCCA loss function
        self.loss_function = GCCA_Loss(self.out_size , self.reg_par).loss

  def _get_outputs(self, x1, x2, x3):
        with torch.no_grad():
            self.arch.eval()
            data_size = x1.size(0)
            batch_idxs = list(BatchSampler(SequentialSampler(
                range(data_size)), batch_size=self.batch_size, drop_last=False))
            losses = []
            outputs1 = []
            outputs2 = []
            outputs3 = []
            for x in batch_idxs:
                batch_x1 = x1[x, :]
                batch_x2 = x2[x, :]
                batch_x3 = x3[x, :]
                o1, o2, o3 = self.arch(batch_x1, batch_x2, batch_x3)
                outputs1.append(o1)
                outputs2.append(o2)
                outputs3.append(o3)
                loss = self.loss_function(o1, o2, o3)
                losses.append(loss.item())
        outputs = [torch.cat(outputs1, dim=0).numpy(),
                   torch.cat(outputs2, dim=0).numpy(),
                   torch.cat(outputs3, dim=0).numpy()]
        return losses, outputs

  def test(self, x1, x2, x3):
        with torch.no_grad():
            losses, outputs = self._get_outputs(x1, x2, x3)
            return np.mean(losses), outputs

  def fit(self, train_x1, train_x2, train_x3, test_x1, test_x2, test_x3):
      data_size = train_x1.size(0)
      train_losses = []
      for epoch in range(self.epoch_num):
          self.arch.train()
          batch_idxs = list(BatchSampler(RandomSampler(
              range(data_size)), batch_size=self.batch_size, drop_last=False))
          for x in batch_idxs:
              self.optimizer.zero_grad()           
              batch_x1 = train_x1[x, :]
              batch_x2 = train_x2[x, :]
              batch_x3 = train_x3[x, :] 
              o1, o2, o3 = self.arch(batch_x1, batch_x2, batch_x3)
              loss = self.loss_function(o1, o2, o3)
              train_losses.append(loss.data)
              loss.backward()
              self.optimizer.step()

          # training the gcca model
          _, outputs = self._get_outputs(train_x1, train_x2, train_x3)
          GCCA_obj = GCCA(self.out_size, method="gcca")
          GCCA_obj.fit(outputs[0], outputs[1], outputs[2],params=None) #function from cca-zoo

          loss = self.test(test_x1, test_x2, test_x3)
      print("Fitted Model to Data")

X1 = torch.randn((20, 3))
X2 = torch.randn((20, 3))
X3 = torch.randn((20, 3))

T1 = torch.randn((2, 3))
T2 = torch.randn((2, 3))
T3 = torch.randn((2, 3))

layer_sizes1 = [3, 10, 10, 2]
layer_sizes2 = [3, 10, 10, 2]
layer_sizes3 = [3, 10, 10, 2]

model = DGCCA_architecture(layer_sizes1, layer_sizes2, layer_sizes3, "sigmoid")

learning_rate = 1e-3
epoch_num = 100
batch_size = 800
reg_par = 1e-5

algo = DGCCA(model, learning_rate, epoch_num, batch_size, reg_par, 20)
algo.fit(X1,X2,X3,T1,T2,T3)
#loss = algo.test(X1,X2,X3)
#print(loss)